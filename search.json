[
  {
    "objectID": "twitterscraper.html",
    "href": "twitterscraper.html",
    "title": "twitter",
    "section": "",
    "text": "source\n\nscrape_twitter_page\n\n scrape_twitter_page (driver, url, scroll_limit=10, sleep_time=2)\n\n\nsource\n\n\nextract_tweets_from_file\n\n extract_tweets_from_file (file_path)"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "foo\n\n foo ()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "adu",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "adu",
    "section": "Install",
    "text": "Install\npip install adu4"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "adu",
    "section": "How to use",
    "text": "How to use\nFirst collect html pages, than run bs4 to process data\n\n# Create a Selenium web driver instance (e.g., using Chrome)\nfrom selenium import webdriver\nimport time\nfrom selenium import webdriver\ndriver = webdriver.Chrome()\n\n# Set the desired Twitter page URL\ntwitter_url = \"https://twitter.com/LulaOficial\"  # Replace with the actual URL of the Twitter page\n\n# Scrape the Twitter page\nscrape_twitter_page(driver, twitter_url, scroll_limit=3, sleep_time=15) #sleep time is important to give time to each log in per driver session\n\n# Close the web driver\n#driver.quit()\n\n\nimport glob\nfrom bs4 import BeautifulSoup\n\nfile_paths = glob.glob('*.html')\n\nfor file_path in file_paths:\n    print(file_path)\n    tweets = extract_tweets_from_file(file_path)    \n    for tweet in tweets:\n            print(file_path, tweet)"
  }
]